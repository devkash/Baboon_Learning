{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "CNN with Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C86hAhFE56EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Activation\n",
        "from keras.layers import MaxPooling1D, UpSampling1D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten, Reshape\n",
        "from keras.layers import Input, Dense, Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rand\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import keras\n",
        "import statistics as st\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import p_f1_recall as metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MbgdZSj56EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uySvGzj56Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###3 HYPERPARAMETERS ####\n",
        "batch_size, epochs, verbose = 200, 1, 1\n",
        "\n",
        "group_size = 48\n",
        "num_connected_nodes = 10\n",
        "kernal_size = 6\n",
        "num_filters = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FCTkpjd56Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### First proces unlabled data\n",
        "\n",
        "data_u = pd.read_csv('fullest_unlabeled_data.csv')\n",
        "\n",
        "### Remove unncessary columns\n",
        "data_u = data_u.iloc[:,[3,4,5,7,8,9,10]]\n",
        "\n",
        "### Standardize the features\n",
        "def standardize(data,col_nums_to_standardize):\n",
        "    for col in col_nums_to_standardize:\n",
        "        data.iloc[:,col] = (data.iloc[:,col] - np.mean(data.iloc[:,col]))/np.std(data.iloc[:,col])\n",
        "    return data\n",
        "\n",
        "data_u = standardize(data_u,col_nums_to_standardize = range(6))\n",
        "\n",
        "\n",
        "#### Now process the labeled data\n",
        "data_train = pd.read_csv('final_training.csv')\n",
        "data_test = pd.read_csv('final_testing.csv')\n",
        "\n",
        "### Remove unncessary columns\n",
        "data_train = data_train.iloc[:,[3,4,5,7,8,9,10,12]]  ## Add 13 if need to keep IDs ## Drop heading because of low mutual information\n",
        "data_test = data_test.iloc[:,[3,4,5,7,8,9,10,12]]\n",
        "\n",
        "### Standardize the features\n",
        "data_train = standardize(data_train,col_nums_to_standardize = range(6))\n",
        "data_test = standardize(data_test,col_nums_to_standardize = range(6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0p6E2UE56Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Break features into groups of size \n",
        "\n",
        "def break_into_samples(data,group_size):\n",
        "    l = []\n",
        "    sep_ts = set(data.loc[:,'ID'])\n",
        "    for ts in sep_ts:\n",
        "        sub_data  = np.array(data.loc[data.loc[:,'ID']== ts,:])\n",
        "        a = int(sub_data.shape[0]/group_size)\n",
        "        if a != 0:\n",
        "            sub_data = sub_data[:(a*group_size),:]\n",
        "            sub_data = [np.split(sub_data,a)]\n",
        "            l = l + sub_data\n",
        "    data = np.vstack(l)\n",
        "    data = data[:,:,:-1]\n",
        "    return data\n",
        "\n",
        "chunked_data_train = break_into_samples(data_train,group_size)\n",
        "chunked_data_test = break_into_samples(data_test,group_size)\n",
        "chunked_u_data = break_into_samples(data_u,group_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOevSWKJ56Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_val(data,labeled = True,thresh = 0.8):\n",
        "    np.random.shuffle(data)\n",
        "    thresh1 = int(data.shape[0]*thresh)\n",
        "    training = data[:thresh1,:,:]\n",
        "    val = data[thresh1:,:,:]\n",
        "    return training, val\n",
        "\n",
        "\n",
        "global_num_classes = -1\n",
        "\n",
        "def split_feat_lab(data,many_to_one = True, prop = 0.65):\n",
        "    features = data[:,:,:data.shape[2]-1]\n",
        "    labels = data[:,:,data.shape[2]-1]\n",
        "    if many_to_one == True:\n",
        "        y_temp = np.zeros(labels.shape[0])\n",
        "        y_temp.fill(np.nan)\n",
        "        for i in range(labels.shape[0]):\n",
        "            row = labels[i,:]\n",
        "            trans_class = (Counter(row)).most_common(1)[0]\n",
        "            if trans_class[1] < group_size*prop : \n",
        "                 y_temp[i] = 4\n",
        "            else:\n",
        "                 y_temp[i] = st.mode(row)\n",
        "        labels = y_temp\n",
        "        global global_num_classes\n",
        "    if global_num_classes < 0:\n",
        "        global_num_classes = len(set(labels.flatten('C')))\n",
        "    num_classes = global_num_classes\n",
        "    labels = to_categorical(labels,num_classes)\n",
        "    return features,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-lSynm256Ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunked_data_train1, chunked_data_val = split_train_val(chunked_data_train)\n",
        "training, val = split_train_val(chunked_u_data)\n",
        "\n",
        "train_x,train_y = split_feat_lab(chunked_data_train1)\n",
        "val_x,val_y = split_feat_lab(chunked_data_val)\n",
        "test_x,test_y = split_feat_lab(chunked_data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv1pkAIK56Ev",
        "colab_type": "code",
        "colab": {},
        "outputId": "e11e714f-b0e3-4fc9-b8ea-3d17c1f384fe"
      },
      "source": [
        "### CNN auto-encoder ###\n",
        "\n",
        "n_timesteps = training.shape[1]\n",
        "n_features = training.shape[2]\n",
        "    \n",
        "def build_autoencoder():\n",
        "    raw_input = Input(shape=(n_timesteps,n_features))  \n",
        "    encoding_1 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(raw_input)\n",
        "    encoding_2 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(encoding_1)\n",
        "    encoding_3 = MaxPooling1D(3, padding='same')(encoding_2)\n",
        "    encoding_4 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(encoding_3)\n",
        "    encoding_5 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(encoding_4)\n",
        "    encoding_6 = MaxPooling1D(2)(encoding_5)\n",
        "    encoded = Dropout(0.5)(encoding_6)\n",
        "    decoding_1 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(encoded)\n",
        "    decoding_2 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(decoding_1)\n",
        "    decoding_3 = UpSampling1D(size = 2)(decoding_2)\n",
        "    decoding_4 = Conv1D(filters = num_filters, kernel_size = kernal_size, activation='relu', padding='same')(decoding_3)\n",
        "    decoding_5 = Conv1D(filters = n_features, kernel_size = kernal_size, activation='relu', padding='same')(decoding_4)\n",
        "    decoded = UpSampling1D(size = 3)(decoding_5)\n",
        "    encoder = Model(raw_input, encoded)\n",
        "    autoencoder = Model(raw_input, decoded)\n",
        "    return autoencoder, encoder\n",
        "    \n",
        "autoencoder, encoder = build_autoencoder()\n",
        "init_weights = encoder.get_weights()\n",
        "\n",
        "def autoencoder_train(autoencoder):\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    autoencoder.fit(training, training,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(val, val),\n",
        "                callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=True)])\n",
        "    loss = autoencoder.evaluate(val, val, batch_size=batch_size, verbose=verbose)\n",
        "    return loss\n",
        "\n",
        "autoencoder_loss = autoencoder_train(autoencoder)\n",
        "\n",
        "trained_weights = encoder.get_weights()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 116447 samples, validate on 29112 samples\n",
            "Epoch 1/1\n",
            "116447/116447 [==============================] - 59s 505us/step - loss: 0.5596 - val_loss: 0.5315\n",
            "29112/29112 [==============================] - 4s 145us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuTDAKFl56E0",
        "colab_type": "code",
        "colab": {},
        "outputId": "89fe03aa-4f19-42ea-9e66-11ecba0acb93"
      },
      "source": [
        "encoder.set_weights(trained_weights)\n",
        "new_layer1 = Flatten()(encoder.output) ## Could try GlobalAveragePool here instead\n",
        "new_layer2 = Dense(num_connected_nodes, activation = 'relu')(new_layer1)\n",
        "new_layer3 = Dense(global_num_classes, activation='softmax')(new_layer2)\n",
        "pre_trained = Model(encoder.input,new_layer3)\n",
        "pre_trained.summary()\n",
        "\n",
        "def train_model(model):\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',metric.recall,metric.precision,metric.f1])\n",
        "    model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size,verbose=verbose,validation_data=(val_x,val_y),\n",
        "             callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=True)])\n",
        "    loss,accuracy,recall_score,precision_score,f_score = model.evaluate(val_x, val_y, batch_size=batch_size, verbose=verbose)\n",
        "    #final_loss,final_acc = model.evaluate(test_features_x, test_features_y, batch_size = batch_size, verbose=verbose)\n",
        "    return loss,accuracy,recall_score,precision_score,f_score # . ,final_loss,final_acc\n",
        "\n",
        "m1_loss,m1_accuracy,m1_recall_score,m1_precision_score,m1_f_score = train_model(pre_trained)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 48, 6)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 48, 64)            2368      \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 48, 64)            24640     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 16, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 16, 64)            24640     \n",
            "_________________________________________________________________\n",
            "conv1d_20 (Conv1D)           (None, 16, 64)            24640     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 8, 64)             0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 64)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 81,473\n",
            "Trainable params: 81,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 969 samples, validate on 243 samples\n",
            "Epoch 1/1\n",
            "969/969 [==============================] - 1s 1ms/step - loss: 1.4899 - acc: 0.4159 - recall: 0.0217 - precision: 0.2856 - f1: 0.0386 - val_loss: 1.2076 - val_acc: 0.6049 - val_recall: 0.2757 - val_precision: 0.7792 - val_f1: 0.4072\n",
            "243/243 [==============================] - 0s 143us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2075842991777899,\n",
              " 0.6049382843598416,\n",
              " 0.2757201660807731,\n",
              " 0.779182188304854,\n",
              " 0.40719928270504796)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybdu15Et56E3",
        "colab_type": "code",
        "colab": {},
        "outputId": "d3ea6178-7dcc-4ad4-ca21-f7ad7cc3e63b"
      },
      "source": [
        "encoder.set_weights(init_weights)\n",
        "new_layer1 = Flatten()(encoder.output) ## Could try GlobalAveragePool here instead\n",
        "new_layer2 = Dense(num_connected_nodes, activation = 'relu')(new_layer1)\n",
        "new_layer3 = Dense(global_num_classes, activation='softmax')(new_layer2)\n",
        "no_pre_training = Model(encoder.input,new_layer3)  ## This just makes a model without the encoding\n",
        "\n",
        "m2_loss,m2_accuracy,m2_recall_score,m2_precision_score,m2_f_score = train_model(no_pre_training)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 969 samples, validate on 243 samples\n",
            "Epoch 1/1\n",
            "969/969 [==============================] - 1s 1ms/step - loss: 1.4895 - acc: 0.3829 - recall: 0.0268 - precision: 0.1834 - f1: 0.0453 - val_loss: 1.3069 - val_acc: 0.4856 - val_recall: 0.2716 - val_precision: 0.6665 - val_f1: 0.3859\n",
            "243/243 [==============================] - 0s 100us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.3068794792081102,\n",
              " 0.4855967179983241,\n",
              " 0.2716049436679102,\n",
              " 0.6665426953829856,\n",
              " 0.38592939428341244)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MryW4Ac56E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append([autoencoder_loss,m1_f_score,m2_f_score])\n",
        "print(np.mean(results,axis=0))\n",
        "print(np.sd(results,axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk1ufclK56E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}