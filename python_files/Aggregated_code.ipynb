{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import keras\n",
    "import statistics as st\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/carterloftus/Documents/ECS 289/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for F1 score (obtained online, have to cite), which we will use in the models later\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120576/120576 [==============================] - 12s 98us/step - loss: 0.6151 - acc: 0.7834\n",
      "Epoch 2/10\n",
      "120576/120576 [==============================] - 8s 66us/step - loss: 0.5862 - acc: 0.7894\n",
      "Epoch 3/10\n",
      "120576/120576 [==============================] - 8s 66us/step - loss: 0.5740 - acc: 0.7914\n",
      "Epoch 4/10\n",
      "120576/120576 [==============================] - 8s 64us/step - loss: 0.5647 - acc: 0.7932\n",
      "Epoch 5/10\n",
      "120576/120576 [==============================] - 8s 70us/step - loss: 0.5568 - acc: 0.7951\n",
      "Epoch 6/10\n",
      "120576/120576 [==============================] - 9s 74us/step - loss: 0.5503 - acc: 0.7971\n",
      "Epoch 7/10\n",
      "120576/120576 [==============================] - 10s 84us/step - loss: 0.5430 - acc: 0.7979\n",
      "Epoch 8/10\n",
      "120576/120576 [==============================] - 10s 79us/step - loss: 0.5378 - acc: 0.7996\n",
      "Epoch 9/10\n",
      "120576/120576 [==============================] - 9s 76us/step - loss: 0.5339 - acc: 0.8011\n",
      "Epoch 10/10\n",
      "120576/120576 [==============================] - 8s 68us/step - loss: 0.5291 - acc: 0.8022\n",
      "30108/30108 [==============================] - 2s 75us/step\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv('train_df.csv')\n",
    "val_data = pd.read_csv('val_df.csv')\n",
    "\n",
    "### Remove unncessary columns\n",
    "data_train = training_data.iloc[:,[4,5,6,7,8,9,10,11,13]]\n",
    "data_val = val_data.iloc[:,[4,5,6,7,8,9,10,11,13]]\n",
    "\n",
    "### Change -1 labels to 0\n",
    "#data_train.loc[(data_train.loc[:,'LABEL_O']== -1),'LABEL_O'] = 0\n",
    "#data_val.loc[(data_val.loc[:,'LABEL_O']== -1),'LABEL_O'] = 0\n",
    "\n",
    "### Or just drop the unknown class (-1)\n",
    "data_train = data_train[~data_train['LABEL_O'].isin([-1])]\n",
    "data_val = data_val[~data_val['LABEL_O'].isin([-1])]\n",
    "### And then reduce the value of the other categories to start at 0\n",
    "data_train['LABEL_O'] = data_train['LABEL_O']-1\n",
    "data_val['LABEL_O'] = data_val['LABEL_O']-1\n",
    "\n",
    "### Standardize the features\n",
    "def standardize(data,col_nums_to_standardize):\n",
    "    for col in col_nums_to_standardize:\n",
    "        data.iloc[:,col] = (data.iloc[:,col] - np.mean(data.iloc[:,col]))/np.std(data.iloc[:,col])\n",
    "    return data\n",
    "\n",
    "data_train = standardize(data_train,col_nums_to_standardize = range(7))\n",
    "data_val = standardize(data_val,col_nums_to_standardize = range(7))\n",
    "\n",
    "## Extremely basis NN with no aggregation of time:\n",
    "\n",
    "epochs, batch_size, verbose = 10,64,1\n",
    "\n",
    "\n",
    "train_x = np.array(data_train.iloc[:,:-2])\n",
    "train_y = np.array(data_train.iloc[:,-2])\n",
    "train_y = to_categorical(train_y)\n",
    "val_x = np.array(data_val.iloc[:,:-2])\n",
    "val_y = np.array(data_val.iloc[:,-2])\n",
    "val_y = to_categorical(val_y)\n",
    "\n",
    "def build_basicDNN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100,activation = 'relu',input_dim = train_x.shape[1]))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "    \n",
    "def train_model(model):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size,verbose=verbose)\n",
    "    loss,accuracy = model.evaluate(val_x, val_y, batch_size=batch_size, verbose=verbose)\n",
    "    #final_loss,final_acc = model.evaluate(test_features_x, test_features_y, batch_size = batch_size, verbose=verbose)\n",
    "    return loss,accuracy # . ,final_loss,final_acc\n",
    "\n",
    "model = build_basicDNN()\n",
    "loss,accuracy = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break features into groups of size n\n",
    "group_size = 6\n",
    "\n",
    "def break_into_samples(features,group_size):\n",
    "    l = []\n",
    "    sep_ts = set(features.loc[:,'ID'])\n",
    "    for ts in sep_ts:\n",
    "        sub_data  = np.array(features.loc[features.loc[:,'ID']== ts,:])\n",
    "        a = int(sub_data.shape[0]/group_size)\n",
    "        if a != 0:\n",
    "            sub_data = sub_data[:(a*group_size),:]\n",
    "            sub_data = [np.split(sub_data,a)]\n",
    "            l = l + sub_data\n",
    "    features = np.vstack(l)\n",
    "    features = features[:,:,:-1]\n",
    "    return features\n",
    "\n",
    "data_train = break_into_samples(data_train,group_size)\n",
    "data_val = break_into_samples(data_val,group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20096/20096 [==============================] - 2s 99us/step - loss: 1.9924 - acc: 0.6881\n",
      "Epoch 2/10\n",
      "20096/20096 [==============================] - 1s 26us/step - loss: 0.8564 - acc: 0.7290\n",
      "Epoch 3/10\n",
      "20096/20096 [==============================] - 1s 25us/step - loss: 0.8152 - acc: 0.7293\n",
      "Epoch 4/10\n",
      "20096/20096 [==============================] - 0s 25us/step - loss: 0.8010 - acc: 0.7295\n",
      "Epoch 5/10\n",
      "20096/20096 [==============================] - 0s 22us/step - loss: 0.7944 - acc: 0.7295\n",
      "Epoch 6/10\n",
      "20096/20096 [==============================] - 0s 22us/step - loss: 0.7903 - acc: 0.7295\n",
      "Epoch 7/10\n",
      "20096/20096 [==============================] - 1s 31us/step - loss: 0.7850 - acc: 0.7295\n",
      "Epoch 8/10\n",
      "20096/20096 [==============================] - 0s 24us/step - loss: 0.7813 - acc: 0.7296\n",
      "Epoch 9/10\n",
      "20096/20096 [==============================] - 1s 26us/step - loss: 0.7749 - acc: 0.7296\n",
      "Epoch 10/10\n",
      "20096/20096 [==============================] - 0s 24us/step - loss: 0.7632 - acc: 0.7296\n",
      "5018/5018 [==============================] - 1s 121us/step\n"
     ]
    }
   ],
   "source": [
    "### If already working with a separated training/validation set, skip down to the next function (split_features_label()), but if not, uncomment this and run it\n",
    "## def split_dataset_train_test(features,thresh = int(features.shape[0]*0.75), many_to_one = True, prop = 0.65):\n",
    "#    np.random.shuffle(features)\n",
    "#    train_x = features[:thresh,:,:features.shape[2]-1]\n",
    "#    train_y = features[:thresh,:,features.shape[2]-1]\n",
    "#    if many_to_one == True:\n",
    "#         y_temp = np.zeros(train_y.shape[0])\n",
    "#         y_temp.fill(np.nan)\n",
    "#         for i in range(y.shape[0]):\n",
    "#             row = train_y[i,:]\n",
    "#             trans_class = (Counter(row)).most_common(1)[0]\n",
    "#             if trans_class[1] < group_size*prop : \n",
    "#                 y_temp[i] = 4\n",
    "#             else:\n",
    "#                 y_temp[i] = st.mode(row)\n",
    "#         train_y = y_temp\n",
    "#    num_classes = len(set(train_y))\n",
    "#    train_y = to_categorical(train_y,num_classes)\n",
    "#    val_x = features[thresh:,:,:features.shape[2]-1]\n",
    "#    val_y = features[thresh:,:,features.shape[2]-1]\n",
    "#    if many_to_one == True:\n",
    "#         y_temp = np.zeros(val_y.shape[0])\n",
    "#         y_temp.fill(np.nan)\n",
    "#         for i in range(val_y.shape[0]):\n",
    "#             row = val_y[i,:]\n",
    "#             trans_class = (Counter(row)).most_common(1)[0]\n",
    "#             if trans_class[1] < group_size*prop : \n",
    "#                 y_temp[i] = 4\n",
    "#             else:\n",
    "#                 y_temp[i] = st.mode(row)\n",
    "#         val_y = y_temp\n",
    "#    val_y = to_categorical(train_y,num_classes)\n",
    "#    return train_x, train_y, val_x, val_y,num_classes\n",
    "\n",
    "\n",
    "def split_features_label(features,many_to_one = True,prop = 0.65):\n",
    "    np.random.shuffle(features)\n",
    "    x = features[:,:,:features.shape[2]-1]\n",
    "    y = features[:,:,features.shape[2]-1]\n",
    "    if many_to_one == True:\n",
    "        y_temp = np.zeros(y.shape[0])\n",
    "        y_temp.fill(np.nan)\n",
    "        for i in range(y.shape[0]):\n",
    "            row = y[i,:]\n",
    "            trans_class = (Counter(row)).most_common(1)[0]\n",
    "            if trans_class[1] < group_size*prop : \n",
    "                y_temp[i] = 4\n",
    "            else:\n",
    "                y_temp[i] = st.mode(row)\n",
    "        y = y_temp\n",
    "    \n",
    "    num_classes = len(set(y))\n",
    "    y = to_categorical(y,num_classes)\n",
    "    return x,y,num_classes\n",
    "\n",
    "train_x,train_y,num_classes = split_features_label(data_train)\n",
    "val_x,val_y,num_classes = split_features_label(data_val)\n",
    "\n",
    "### Build a basic feedforward DNN by aggregating all inputs in each time segment:\n",
    "\n",
    "train_x_flat = train_x.reshape(train_x.shape[0],(train_x.shape[1] * train_x.shape[2]))\n",
    "val_x_flat = val_x.reshape(val_x.shape[0],(val_x.shape[1] * val_x.shape[2]))\n",
    "\n",
    "def build_DNN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100,activation = 'relu',input_dim = train_x_flat.shape[1]))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "    \n",
    "def train_model(model):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(train_x_flat,train_y,epochs=epochs,batch_size=batch_size,verbose=verbose)\n",
    "    loss,accuracy = model.evaluate(val_x_flat, val_y, batch_size=batch_size, verbose=verbose)\n",
    "    #final_loss,final_acc = model.evaluate(test_features_x, test_features_y, batch_size = batch_size, verbose=verbose)\n",
    "    return loss,accuracy # . ,final_loss,final_acc\n",
    "\n",
    "model = build_DNN()\n",
    "loss,accuracy = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20096/20096 [==============================] - 4s 210us/step - loss: 0.8268 - acc: 0.7237\n",
      "Epoch 2/10\n",
      "20096/20096 [==============================] - 2s 75us/step - loss: 0.6910 - acc: 0.7647\n",
      "Epoch 3/10\n",
      "20096/20096 [==============================] - 1s 73us/step - loss: 0.6450 - acc: 0.7796\n",
      "Epoch 4/10\n",
      "20096/20096 [==============================] - 2s 77us/step - loss: 0.6318 - acc: 0.7826\n",
      "Epoch 5/10\n",
      "20096/20096 [==============================] - 2s 80us/step - loss: 0.6281 - acc: 0.7844\n",
      "Epoch 6/10\n",
      "20096/20096 [==============================] - 2s 76us/step - loss: 0.6220 - acc: 0.7861\n",
      "Epoch 7/10\n",
      "20096/20096 [==============================] - 2s 76us/step - loss: 0.6215 - acc: 0.7859\n",
      "Epoch 8/10\n",
      "20096/20096 [==============================] - 2s 82us/step - loss: 0.6167 - acc: 0.7850\n",
      "Epoch 9/10\n",
      "20096/20096 [==============================] - 2s 80us/step - loss: 0.6157 - acc: 0.7868\n",
      "Epoch 10/10\n",
      "20096/20096 [==============================] - 2s 91us/step - loss: 0.6118 - acc: 0.7880\n",
      "5018/5018 [==============================] - 1s 142us/step\n",
      "Validation loss:  [0.766182533057385, 0.7190115583185277]\n"
     ]
    }
   ],
   "source": [
    "### Now train an LSTM\n",
    "\n",
    "n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "n_sub_groups = 1\n",
    "n_sub_group_timesteps = int(n_timesteps/n_sub_groups)\n",
    "\n",
    "def split_samples_into_sub_samples(features):\n",
    "    features = features.reshape((features.shape[0],n_sub_groups,n_sub_group_timesteps,n_features))\n",
    "    return features\n",
    "\n",
    "\n",
    "train_x = split_samples_into_sub_samples(train_x)\n",
    "val_x = split_samples_into_sub_samples(val_x)\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    #Define CNN Model\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=2, activation=\"relu\"), input_shape=(None,n_sub_group_timesteps,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=2, activation = \"relu\")))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    #Define LSTM Model\n",
    "    model.add(LSTM(100))\n",
    "    #model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def train_model(model):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size,verbose=verbose)\n",
    "    loss,accuracy = model.evaluate(val_x, val_y, batch_size=batch_size, verbose=verbose)\n",
    "    #final_loss,final_acc = model.evaluate(test_features_x, test_features_y, batch_size = batch_size, verbose=verbose)\n",
    "    return loss,accuracy # . ,final_loss,final_acc\n",
    "\n",
    "model = build_model()\n",
    "loss,accuracy = train_model(model)\n",
    "\n",
    "print(\"Validation loss: \",[loss,accuracy])\n",
    "# print(\"Final Testing loss: \",[final_loss,final_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120576 samples, validate on 30108 samples\n",
      "Epoch 1/50\n",
      "120576/120576 [==============================] - 6s 49us/step - loss: 0.5217 - acc: 0.8040 - val_loss: 0.7410 - val_acc: 0.7446\n",
      "Epoch 2/50\n",
      "120576/120576 [==============================] - 6s 50us/step - loss: 0.5193 - acc: 0.8053 - val_loss: 0.7285 - val_acc: 0.7523\n",
      "Epoch 3/50\n",
      "120576/120576 [==============================] - 6s 47us/step - loss: 0.5174 - acc: 0.8048 - val_loss: 0.7155 - val_acc: 0.7585\n",
      "Epoch 4/50\n",
      "110208/120576 [==========================>...] - ETA: 0s - loss: 0.5159 - acc: 0.8059"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-368-d50a173bc23a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 callbacks=[TensorBoard(log_dir='/tmp/model2')])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m### Find plot at TensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Two methods of plotting (first one seems better)\n",
    "\n",
    "### For plots:\n",
    "history = model.fit(train_x, train_y,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_x, val_y),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/model2')])\n",
    "\n",
    "### Find plot at TensorBoard\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation loss: \",[loss,accuracy])\n",
    "# print(\"Final Testing loss: \",[final_loss,final_acc])\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
