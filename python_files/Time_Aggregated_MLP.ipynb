{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Time-Aggregated DNN with Pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRlUU9lWReYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Activation\n",
        "from keras.layers import MaxPooling1D, UpSampling1D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten, Reshape\n",
        "from keras.layers import Input, Dense, Bidirectional\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rand\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "import keras\n",
        "import statistics as st\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import p_f1_recall as metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjY8pZynReY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "111eSbA2ReY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### HYPERPARAMETERS ####\n",
        "batch_size, epochs, verbose = 200, 100, 1\n",
        "group_size = 48\n",
        "encoding_dim = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZlD9QJReY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### First proces unlabled data\n",
        "\n",
        "data_u = pd.read_csv('fullest_unlabeled_data.csv')\n",
        "\n",
        "### Remove unncessary columns\n",
        "data_u = data_u.iloc[:,[3,4,5,7,8,9,10]]\n",
        "\n",
        "### Standardize the features\n",
        "def standardize(data,col_nums_to_standardize):\n",
        "    for col in col_nums_to_standardize:\n",
        "        data.iloc[:,col] = (data.iloc[:,col] - np.mean(data.iloc[:,col]))/np.std(data.iloc[:,col])\n",
        "    return data\n",
        "\n",
        "data_u = standardize(data_u,col_nums_to_standardize = range(6))\n",
        "\n",
        "\n",
        "#### Now process the labeled data\n",
        "data_train = pd.read_csv('final_training.csv')\n",
        "data_test = pd.read_csv('final_testing.csv')\n",
        "\n",
        "### Remove unncessary columns\n",
        "data_train = data_train.iloc[:,[3,4,5,7,8,9,10,12]]  ## Add 13 if need to keep IDs ## Drop heading because of low mutual information\n",
        "data_test = data_test.iloc[:,[3,4,5,7,8,9,10,12]]\n",
        "\n",
        "### Standardize the features\n",
        "data_train = standardize(data_train,col_nums_to_standardize = range(6))\n",
        "data_test = standardize(data_test,col_nums_to_standardize = range(6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3AwHTURReZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Break features into groups of size n\n",
        "\n",
        "def break_into_samples(data,group_size):\n",
        "    l = []\n",
        "    sep_ts = set(data.loc[:,'ID'])\n",
        "    for ts in sep_ts:\n",
        "        sub_data  = np.array(data.loc[data.loc[:,'ID']== ts,:])\n",
        "        a = int(sub_data.shape[0]/group_size)\n",
        "        if a != 0:\n",
        "            sub_data = sub_data[:(a*group_size),:]\n",
        "            sub_data = [np.split(sub_data,a)]\n",
        "            l = l + sub_data\n",
        "    data = np.vstack(l)\n",
        "    data = data[:,:,:-1]\n",
        "    return data\n",
        "\n",
        "chunked_data_train = break_into_samples(data_train,group_size)\n",
        "chunked_data_test = break_into_samples(data_test,group_size)\n",
        "chunked_u_data = break_into_samples(data_u,group_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gF494HeReZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_val(data,labeled = True,thresh = 0.8):\n",
        "    np.random.shuffle(data)\n",
        "    thresh1 = int(data.shape[0]*thresh)\n",
        "    training = data[:thresh1,:,:]\n",
        "    val = data[thresh1:,:,:]\n",
        "    return training, val\n",
        "\n",
        "\n",
        "global_num_classes = -1\n",
        "\n",
        "def split_feat_lab(data,many_to_one = True, prop = 0.65):\n",
        "    features = data[:,:,:data.shape[2]-1]\n",
        "    labels = data[:,:,data.shape[2]-1]\n",
        "    if many_to_one == True:\n",
        "        y_temp = np.zeros(labels.shape[0])\n",
        "        y_temp.fill(np.nan)\n",
        "        for i in range(labels.shape[0]):\n",
        "            row = labels[i,:]\n",
        "            trans_class = (Counter(row)).most_common(1)[0]\n",
        "            if trans_class[1] < group_size*prop : \n",
        "                 y_temp[i] = 4\n",
        "            else:\n",
        "                 y_temp[i] = st.mode(row)\n",
        "        labels = y_temp\n",
        "        global global_num_classes\n",
        "    if global_num_classes < 0:\n",
        "        global_num_classes = len(set(labels))\n",
        "    num_classes = global_num_classes\n",
        "    labels = to_categorical(labels,num_classes)\n",
        "    return features,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh74EaFbReZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunked_data_train1, chunked_data_val = split_train_val(chunked_data_train)\n",
        "training, val = split_train_val(chunked_u_data)\n",
        "\n",
        "train_x,train_y = split_feat_lab(chunked_data_train1)\n",
        "val_x,val_y = split_feat_lab(chunked_data_val)\n",
        "test_x,test_y = split_feat_lab(chunked_data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTSfwO-8ReZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x_flat = train_x.reshape(train_x.shape[0],(train_x.shape[1] * train_x.shape[2]))\n",
        "val_x_flat = val_x.reshape(val_x.shape[0],(val_x.shape[1] * val_x.shape[2]))\n",
        "test_x_flat = test_x.reshape(test_x.shape[0],(test_x.shape[1] * test_x.shape[2]))\n",
        "\n",
        "training_flat = training.reshape(training.shape[0],(training.shape[1]*training.shape[2]))\n",
        "val_flat = val.reshape(val.shape[0],(val.shape[1]*val.shape[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiLyplraReZb",
        "colab_type": "code",
        "colab": {},
        "outputId": "14da3b3b-c3e6-4d1f-d24f-537a223f21ab"
      },
      "source": [
        "\n",
        "def build_autoencoder(encoding_dim = encoding_dim): \n",
        "    raw_input = Input(shape=(training_flat.shape[1],))\n",
        "    encoding_1 = Dense(encoding_dim*4, activation='relu')(raw_input)\n",
        "    encoding_2 = Dense(encoding_dim*2, activation='relu')(encoding_1)\n",
        "    encoded = Dense(encoding_dim, activation='relu')(encoding_2)\n",
        "    decoding_1 = Dense(encoding_dim*2, activation='relu')(encoded)\n",
        "    decoding_2 = Dense(encoding_dim*4, activation='relu')(decoding_1)\n",
        "    decoded = Dense(training_flat.shape[1])(decoding_2)\n",
        "    autoencoder = Model(raw_input, decoded)\n",
        "    encoder = Model(raw_input, encoded)\n",
        "    return autoencoder, encoder\n",
        "    \n",
        "autoencoder, encoder = build_autoencoder()\n",
        "init_weights = encoder.get_weights() ## Saves initial weights for use later\n",
        "\n",
        "def autoencoder_train(autoencoder):\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    autoencoder.fit(training_flat, training_flat,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(val_flat, val_flat),\n",
        "                callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=True)])\n",
        "    loss = autoencoder.evaluate(val_flat, val_flat, batch_size=batch_size, verbose=verbose)\n",
        "    return loss\n",
        "\n",
        "autoencoder_loss = autoencoder_train(autoencoder)\n",
        "autoencoder.summary()\n",
        "\n",
        "trained_weights = encoder.get_weights() ### This allows us to fine tune below, \n",
        "#but then initialize to these weights if we want to fine tune in a different way without retraining our autoencoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 116447 samples, validate on 29112 samples\n",
            "Epoch 1/100\n",
            "116447/116447 [==============================] - 3s 28us/step - loss: 0.3134 - val_loss: 0.1445\n",
            "Epoch 2/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1377 - val_loss: 0.1374\n",
            "Epoch 3/100\n",
            "116447/116447 [==============================] - 2s 17us/step - loss: 0.1340 - val_loss: 0.1349\n",
            "Epoch 4/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1326 - val_loss: 0.1341\n",
            "Epoch 5/100\n",
            "116447/116447 [==============================] - 2s 17us/step - loss: 0.1318 - val_loss: 0.1338\n",
            "Epoch 6/100\n",
            "116447/116447 [==============================] - 2s 17us/step - loss: 0.1241 - val_loss: 0.1202\n",
            "Epoch 7/100\n",
            "116447/116447 [==============================] - 2s 17us/step - loss: 0.1185 - val_loss: 0.1187\n",
            "Epoch 8/100\n",
            "116447/116447 [==============================] - 2s 17us/step - loss: 0.1178 - val_loss: 0.1184\n",
            "Epoch 9/100\n",
            "116447/116447 [==============================] - ETA: 0s - loss: 0.117 - 2s 18us/step - loss: 0.1175 - val_loss: 0.1185\n",
            "Epoch 10/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1173 - val_loss: 0.1179\n",
            "Epoch 11/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1172 - val_loss: 0.1181\n",
            "Epoch 12/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1170 - val_loss: 0.1176\n",
            "Epoch 13/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1170 - val_loss: 0.1178\n",
            "Epoch 14/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1169 - val_loss: 0.1176ETA: 0s - los\n",
            "Epoch 15/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1168 - val_loss: 0.1176\n",
            "Epoch 16/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1169 - val_loss: 0.1175\n",
            "Epoch 17/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1168 - val_loss: 0.1177\n",
            "Epoch 18/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1168 - val_loss: 0.1174\n",
            "Epoch 19/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1167 - val_loss: 0.1174\n",
            "Epoch 20/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1167 - val_loss: 0.1175\n",
            "Epoch 21/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1167 - val_loss: 0.1175\n",
            "Epoch 22/100\n",
            "116447/116447 [==============================] - 2s 18us/step - loss: 0.1167 - val_loss: 0.1174\n",
            "29112/29112 [==============================] - 0s 7us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 32)                9248      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 16)                144       \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 288)               9504      \n",
            "=================================================================\n",
            "Total params: 20,104\n",
            "Trainable params: 20,104\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aopaI51yReZi",
        "colab_type": "code",
        "colab": {},
        "outputId": "ae2933c0-0c34-4497-fcc5-ed1e651fb70e"
      },
      "source": [
        "encoder.set_weights(trained_weights)\n",
        "new_layer = Dense(global_num_classes, activation='softmax')(encoder.output)\n",
        "pre_trained = Model(encoder.input,new_layer)\n",
        "pre_trained.summary()\n",
        "\n",
        "def train_model(model):\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',metric.recall,metric.precision,metric.f1])\n",
        "    model.fit(train_x_flat,train_y,epochs=epochs,batch_size=batch_size,verbose=verbose,validation_data=(val_x_flat,val_y),\n",
        "             callbacks=[EarlyStopping(monitor='val_f1', min_delta=0, patience=7, verbose=0, mode='max', baseline=None, restore_best_weights=True)])\n",
        "    loss,accuracy,recall_score,precision_score,f_score = model.evaluate(val_x_flat, val_y, batch_size=batch_size, verbose=verbose)\n",
        "    return loss,accuracy,recall_score,precision_score,f_score\n",
        "\n",
        "m1_loss,m1_accuracy,m1_recall_score,m1_precision_score,m1_f_score = train_model(pre_trained)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 32)                9248      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 5)                 45        \n",
            "=================================================================\n",
            "Total params: 9,957\n",
            "Trainable params: 9,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 969 samples, validate on 243 samples\n",
            "Epoch 1/100\n",
            "969/969 [==============================] - 1s 884us/step - loss: 5.1492 - acc: 0.2518 - recall: 0.2353 - precision: 0.2526 - f1: 0.2435 - val_loss: 3.7173 - val_acc: 0.3086 - val_recall: 0.2593 - val_precision: 0.3152 - val_f1: 0.2844\n",
            "Epoch 2/100\n",
            "969/969 [==============================] - 0s 16us/step - loss: 3.1962 - acc: 0.3777 - recall: 0.2941 - precision: 0.3946 - f1: 0.3368 - val_loss: 2.6142 - val_acc: 0.4856 - val_recall: 0.4527 - val_precision: 0.5465 - val_f1: 0.4950\n",
            "Epoch 3/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 2.5175 - acc: 0.4685 - recall: 0.4262 - precision: 0.5311 - f1: 0.4729 - val_loss: 2.1000 - val_acc: 0.5432 - val_recall: 0.4774 - val_precision: 0.5783 - val_f1: 0.5228\n",
            "Epoch 4/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 2.0580 - acc: 0.4892 - recall: 0.4469 - precision: 0.5638 - f1: 0.4985 - val_loss: 1.6937 - val_acc: 0.5679 - val_recall: 0.4856 - val_precision: 0.6325 - val_f1: 0.5493\n",
            "Epoch 5/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 1.6846 - acc: 0.5005 - recall: 0.4324 - precision: 0.6517 - f1: 0.5194 - val_loss: 1.4150 - val_acc: 0.5556 - val_recall: 0.4486 - val_precision: 0.7668 - val_f1: 0.5659\n",
            "Epoch 6/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 1.4377 - acc: 0.5170 - recall: 0.4200 - precision: 0.7199 - f1: 0.5301 - val_loss: 1.2806 - val_acc: 0.5514 - val_recall: 0.4156 - val_precision: 0.7829 - val_f1: 0.5430\n",
            "Epoch 7/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 1.3161 - acc: 0.5160 - recall: 0.3953 - precision: 0.7126 - f1: 0.5081 - val_loss: 1.1960 - val_acc: 0.5432 - val_recall: 0.3951 - val_precision: 0.8134 - val_f1: 0.5318\n",
            "Epoch 8/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 1.2107 - acc: 0.5480 - recall: 0.3860 - precision: 0.7352 - f1: 0.5058 - val_loss: 1.0659 - val_acc: 0.6173 - val_recall: 0.4156 - val_precision: 0.8211 - val_f1: 0.5518\n",
            "Epoch 9/100\n",
            "969/969 [==============================] - 0s 16us/step - loss: 1.1214 - acc: 0.6130 - recall: 0.3932 - precision: 0.7586 - f1: 0.5170 - val_loss: 0.9867 - val_acc: 0.6667 - val_recall: 0.4239 - val_precision: 0.8372 - val_f1: 0.5628\n",
            "Epoch 10/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 1.0671 - acc: 0.6388 - recall: 0.4190 - precision: 0.7791 - f1: 0.5440 - val_loss: 0.9472 - val_acc: 0.6872 - val_recall: 0.4609 - val_precision: 0.8293 - val_f1: 0.5923\n",
            "Epoch 11/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 1.0328 - acc: 0.6522 - recall: 0.4572 - precision: 0.7659 - f1: 0.5721 - val_loss: 0.9258 - val_acc: 0.6872 - val_recall: 0.5021 - val_precision: 0.8295 - val_f1: 0.6253\n",
            "Epoch 12/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 1.0097 - acc: 0.6491 - recall: 0.4912 - precision: 0.7606 - f1: 0.5963 - val_loss: 0.9030 - val_acc: 0.6872 - val_recall: 0.5309 - val_precision: 0.8322 - val_f1: 0.6480\n",
            "Epoch 13/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.9878 - acc: 0.6636 - recall: 0.5160 - precision: 0.7662 - f1: 0.6164 - val_loss: 0.8824 - val_acc: 0.6955 - val_recall: 0.5391 - val_precision: 0.8190 - val_f1: 0.6499\n",
            "Epoch 14/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 0.9683 - acc: 0.6687 - recall: 0.5222 - precision: 0.7679 - f1: 0.6216 - val_loss: 0.8700 - val_acc: 0.7037 - val_recall: 0.5514 - val_precision: 0.8171 - val_f1: 0.6585\n",
            "Epoch 15/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.9507 - acc: 0.6729 - recall: 0.5418 - precision: 0.7788 - f1: 0.6382 - val_loss: 0.8606 - val_acc: 0.7078 - val_recall: 0.5638 - val_precision: 0.8354 - val_f1: 0.6732\n",
            "Epoch 16/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.9327 - acc: 0.6749 - recall: 0.5542 - precision: 0.7784 - f1: 0.6471 - val_loss: 0.8573 - val_acc: 0.7119 - val_recall: 0.5720 - val_precision: 0.8277 - val_f1: 0.6764\n",
            "Epoch 17/100\n",
            "969/969 [==============================] - 0s 16us/step - loss: 0.9167 - acc: 0.6791 - recall: 0.5573 - precision: 0.7806 - f1: 0.6502 - val_loss: 0.8472 - val_acc: 0.6996 - val_recall: 0.5802 - val_precision: 0.8297 - val_f1: 0.6826\n",
            "Epoch 18/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.9028 - acc: 0.6863 - recall: 0.5686 - precision: 0.7880 - f1: 0.6600 - val_loss: 0.8385 - val_acc: 0.7037 - val_recall: 0.5802 - val_precision: 0.8247 - val_f1: 0.6809\n",
            "Epoch 19/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.8874 - acc: 0.6852 - recall: 0.5759 - precision: 0.7855 - f1: 0.6643 - val_loss: 0.8349 - val_acc: 0.6996 - val_recall: 0.5926 - val_precision: 0.8229 - val_f1: 0.6889\n",
            "Epoch 20/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.8746 - acc: 0.6811 - recall: 0.5800 - precision: 0.7808 - f1: 0.6652 - val_loss: 0.8298 - val_acc: 0.6996 - val_recall: 0.5926 - val_precision: 0.8187 - val_f1: 0.6872\n",
            "Epoch 21/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.8638 - acc: 0.6894 - recall: 0.5851 - precision: 0.7805 - f1: 0.6687 - val_loss: 0.8232 - val_acc: 0.7119 - val_recall: 0.5967 - val_precision: 0.8233 - val_f1: 0.6916\n",
            "Epoch 22/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.8468 - acc: 0.6914 - recall: 0.5810 - precision: 0.7830 - f1: 0.6668 - val_loss: 0.8301 - val_acc: 0.6996 - val_recall: 0.5967 - val_precision: 0.8189 - val_f1: 0.6902\n",
            "Epoch 23/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.8357 - acc: 0.6976 - recall: 0.5810 - precision: 0.7847 - f1: 0.6674 - val_loss: 0.8305 - val_acc: 0.7078 - val_recall: 0.5926 - val_precision: 0.8178 - val_f1: 0.6871\n",
            "Epoch 24/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 0.8219 - acc: 0.6956 - recall: 0.5810 - precision: 0.7843 - f1: 0.6674 - val_loss: 0.8197 - val_acc: 0.7037 - val_recall: 0.5967 - val_precision: 0.8186 - val_f1: 0.6900\n",
            "Epoch 25/100\n",
            "969/969 [==============================] - 0s 16us/step - loss: 0.8122 - acc: 0.7049 - recall: 0.5893 - precision: 0.7845 - f1: 0.6730 - val_loss: 0.8173 - val_acc: 0.6996 - val_recall: 0.6049 - val_precision: 0.8204 - val_f1: 0.6960\n",
            "Epoch 26/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.7998 - acc: 0.7038 - recall: 0.5924 - precision: 0.7819 - f1: 0.6738 - val_loss: 0.8173 - val_acc: 0.7037 - val_recall: 0.6049 - val_precision: 0.7981 - val_f1: 0.6878\n",
            "Epoch 27/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.7908 - acc: 0.7007 - recall: 0.6017 - precision: 0.7845 - f1: 0.6806 - val_loss: 0.8069 - val_acc: 0.7119 - val_recall: 0.5967 - val_precision: 0.8094 - val_f1: 0.6863\n",
            "Epoch 28/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.7806 - acc: 0.7110 - recall: 0.6078 - precision: 0.7854 - f1: 0.6851 - val_loss: 0.7994 - val_acc: 0.7160 - val_recall: 0.6049 - val_precision: 0.8162 - val_f1: 0.6943\n",
            "Epoch 29/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.7693 - acc: 0.7162 - recall: 0.6099 - precision: 0.7904 - f1: 0.6885 - val_loss: 0.8009 - val_acc: 0.7243 - val_recall: 0.6091 - val_precision: 0.8127 - val_f1: 0.6958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.7601 - acc: 0.7172 - recall: 0.6171 - precision: 0.7921 - f1: 0.6936 - val_loss: 0.7965 - val_acc: 0.7202 - val_recall: 0.6091 - val_precision: 0.7993 - val_f1: 0.6910\n",
            "Epoch 31/100\n",
            "969/969 [==============================] - 0s 15us/step - loss: 0.7542 - acc: 0.7172 - recall: 0.6192 - precision: 0.7821 - f1: 0.6906 - val_loss: 0.7854 - val_acc: 0.7243 - val_recall: 0.6091 - val_precision: 0.8081 - val_f1: 0.6942\n",
            "Epoch 32/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.7455 - acc: 0.7224 - recall: 0.6295 - precision: 0.7839 - f1: 0.6982 - val_loss: 0.7877 - val_acc: 0.7119 - val_recall: 0.6049 - val_precision: 0.7937 - val_f1: 0.6862\n",
            "243/243 [==============================] - 0s 12us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajib7sSyReZp",
        "colab_type": "code",
        "colab": {},
        "outputId": "cb7f7ba8-116c-4408-8af2-a4fa15f6d63a"
      },
      "source": [
        "encoder.set_weights(init_weights) ### Reinitializes the encoder\n",
        "new_layer = Dense(global_num_classes, activation='softmax')(encoder.output)\n",
        "no_pre_training = Model(encoder.input,new_layer)  ## This just makes a model without the encoding\n",
        "\n",
        "m2_loss,m2_accuracy,m2_recall_score,m2_precision_score,m2_f_score= train_model(no_pre_training)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 969 samples, validate on 243 samples\n",
            "Epoch 1/100\n",
            "969/969 [==============================] - 1s 932us/step - loss: 1.6010 - acc: 0.2270 - recall: 0.0000e+00 - precision: 0.0000e+00 - f1: 0.0000e+00 - val_loss: 1.5026 - val_acc: 0.4280 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 1.4790 - acc: 0.4272 - recall: 0.0010 - precision: 0.1744 - f1: 0.0021 - val_loss: 1.4116 - val_acc: 0.5267 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_f1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 1.3889 - acc: 0.5088 - recall: 0.0134 - precision: 0.7413 - f1: 0.0259 - val_loss: 1.3210 - val_acc: 0.5638 - val_recall: 0.0247 - val_precision: 0.6173 - val_f1: 0.0475\n",
            "Epoch 4/100\n",
            "969/969 [==============================] - 0s 24us/step - loss: 1.3032 - acc: 0.5666 - recall: 0.1011 - precision: 0.8066 - f1: 0.1754 - val_loss: 1.2396 - val_acc: 0.5844 - val_recall: 0.2510 - val_precision: 0.8581 - val_f1: 0.3882\n",
            "Epoch 5/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 1.2315 - acc: 0.5769 - recall: 0.2528 - precision: 0.8422 - f1: 0.3875 - val_loss: 1.1767 - val_acc: 0.6091 - val_recall: 0.3251 - val_precision: 0.8681 - val_f1: 0.4730\n",
            "Epoch 6/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 1.1723 - acc: 0.5903 - recall: 0.3096 - precision: 0.8308 - f1: 0.4505 - val_loss: 1.1280 - val_acc: 0.6173 - val_recall: 0.3416 - val_precision: 0.8383 - val_f1: 0.4853\n",
            "Epoch 7/100\n",
            "969/969 [==============================] - 0s 26us/step - loss: 1.1267 - acc: 0.6068 - recall: 0.3271 - precision: 0.8160 - f1: 0.4667 - val_loss: 1.0940 - val_acc: 0.6337 - val_recall: 0.3539 - val_precision: 0.8354 - val_f1: 0.4971\n",
            "Epoch 8/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 1.0885 - acc: 0.6192 - recall: 0.3375 - precision: 0.7961 - f1: 0.4734 - val_loss: 1.0611 - val_acc: 0.6337 - val_recall: 0.3827 - val_precision: 0.8461 - val_f1: 0.5269\n",
            "Epoch 9/100\n",
            "969/969 [==============================] - 0s 27us/step - loss: 1.0494 - acc: 0.6192 - recall: 0.3633 - precision: 0.8032 - f1: 0.4997 - val_loss: 1.0394 - val_acc: 0.6214 - val_recall: 0.3992 - val_precision: 0.8515 - val_f1: 0.5434\n",
            "Epoch 10/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 1.0150 - acc: 0.6233 - recall: 0.4004 - precision: 0.8126 - f1: 0.5360 - val_loss: 1.0193 - val_acc: 0.6214 - val_recall: 0.4033 - val_precision: 0.8316 - val_f1: 0.5429\n",
            "Epoch 11/100\n",
            "969/969 [==============================] - 0s 25us/step - loss: 0.9898 - acc: 0.6326 - recall: 0.4107 - precision: 0.8076 - f1: 0.5439 - val_loss: 0.9985 - val_acc: 0.6091 - val_recall: 0.4115 - val_precision: 0.8345 - val_f1: 0.5508\n",
            "Epoch 12/100\n",
            "969/969 [==============================] - 0s 23us/step - loss: 0.9671 - acc: 0.6336 - recall: 0.4159 - precision: 0.8143 - f1: 0.5503 - val_loss: 0.9819 - val_acc: 0.6379 - val_recall: 0.4198 - val_precision: 0.8301 - val_f1: 0.5573\n",
            "Epoch 13/100\n",
            "969/969 [==============================] - 0s 25us/step - loss: 0.9440 - acc: 0.6388 - recall: 0.4324 - precision: 0.8096 - f1: 0.5634 - val_loss: 0.9697 - val_acc: 0.6420 - val_recall: 0.4280 - val_precision: 0.8260 - val_f1: 0.5637\n",
            "Epoch 14/100\n",
            "969/969 [==============================] - 0s 24us/step - loss: 0.9242 - acc: 0.6440 - recall: 0.4582 - precision: 0.7981 - f1: 0.5819 - val_loss: 0.9560 - val_acc: 0.6379 - val_recall: 0.4609 - val_precision: 0.8238 - val_f1: 0.5909\n",
            "Epoch 15/100\n",
            "969/969 [==============================] - 0s 23us/step - loss: 0.9052 - acc: 0.6440 - recall: 0.4727 - precision: 0.8101 - f1: 0.5965 - val_loss: 0.9383 - val_acc: 0.6543 - val_recall: 0.4527 - val_precision: 0.8275 - val_f1: 0.5849\n",
            "Epoch 16/100\n",
            "969/969 [==============================] - 0s 23us/step - loss: 0.8871 - acc: 0.6522 - recall: 0.4747 - precision: 0.8173 - f1: 0.6005 - val_loss: 0.9288 - val_acc: 0.6420 - val_recall: 0.4650 - val_precision: 0.8506 - val_f1: 0.6010\n",
            "Epoch 17/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.8709 - acc: 0.6594 - recall: 0.4923 - precision: 0.8194 - f1: 0.6150 - val_loss: 0.9191 - val_acc: 0.6379 - val_recall: 0.4733 - val_precision: 0.8474 - val_f1: 0.6069\n",
            "Epoch 18/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.8573 - acc: 0.6646 - recall: 0.5088 - precision: 0.8247 - f1: 0.6292 - val_loss: 0.9087 - val_acc: 0.6420 - val_recall: 0.4979 - val_precision: 0.8476 - val_f1: 0.6270\n",
            "Epoch 19/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.8413 - acc: 0.6749 - recall: 0.5212 - precision: 0.8209 - f1: 0.6372 - val_loss: 0.9051 - val_acc: 0.6296 - val_recall: 0.5185 - val_precision: 0.8519 - val_f1: 0.6443\n",
            "Epoch 20/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 0.8269 - acc: 0.6739 - recall: 0.5397 - precision: 0.8278 - f1: 0.6533 - val_loss: 0.8911 - val_acc: 0.6337 - val_recall: 0.5226 - val_precision: 0.8529 - val_f1: 0.6478\n",
            "Epoch 21/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 0.8146 - acc: 0.6904 - recall: 0.5490 - precision: 0.8252 - f1: 0.6589 - val_loss: 0.8843 - val_acc: 0.6379 - val_recall: 0.5226 - val_precision: 0.8471 - val_f1: 0.6462\n",
            "Epoch 22/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.7986 - acc: 0.6976 - recall: 0.5552 - precision: 0.8353 - f1: 0.6669 - val_loss: 0.8796 - val_acc: 0.6420 - val_recall: 0.5350 - val_precision: 0.8676 - val_f1: 0.6613\n",
            "Epoch 23/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 0.7881 - acc: 0.7028 - recall: 0.5635 - precision: 0.8348 - f1: 0.6724 - val_loss: 0.8737 - val_acc: 0.6667 - val_recall: 0.5226 - val_precision: 0.8589 - val_f1: 0.6492\n",
            "Epoch 24/100\n",
            "969/969 [==============================] - 0s 15us/step - loss: 0.7773 - acc: 0.7018 - recall: 0.5697 - precision: 0.8337 - f1: 0.6766 - val_loss: 0.8628 - val_acc: 0.6626 - val_recall: 0.5350 - val_precision: 0.8617 - val_f1: 0.6596\n",
            "Epoch 25/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 0.7625 - acc: 0.7059 - recall: 0.5717 - precision: 0.8317 - f1: 0.6773 - val_loss: 0.8617 - val_acc: 0.6790 - val_recall: 0.5391 - val_precision: 0.8568 - val_f1: 0.6614\n",
            "Epoch 26/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.7528 - acc: 0.7172 - recall: 0.5789 - precision: 0.8358 - f1: 0.6839 - val_loss: 0.8553 - val_acc: 0.6790 - val_recall: 0.5350 - val_precision: 0.8626 - val_f1: 0.6599\n",
            "Epoch 27/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.7416 - acc: 0.7183 - recall: 0.5841 - precision: 0.8392 - f1: 0.6883 - val_loss: 0.8548 - val_acc: 0.6667 - val_recall: 0.5350 - val_precision: 0.8444 - val_f1: 0.6548\n",
            "Epoch 28/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.7308 - acc: 0.7245 - recall: 0.5903 - precision: 0.8427 - f1: 0.6939 - val_loss: 0.8514 - val_acc: 0.6584 - val_recall: 0.5432 - val_precision: 0.8464 - val_f1: 0.6616\n",
            "Epoch 29/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.7173 - acc: 0.7234 - recall: 0.5965 - precision: 0.8412 - f1: 0.6977 - val_loss: 0.8469 - val_acc: 0.6708 - val_recall: 0.5473 - val_precision: 0.8473 - val_f1: 0.6646\n",
            "Epoch 30/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.7081 - acc: 0.7348 - recall: 0.6047 - precision: 0.8388 - f1: 0.7027 - val_loss: 0.8410 - val_acc: 0.6831 - val_recall: 0.5514 - val_precision: 0.8223 - val_f1: 0.6598\n",
            "Epoch 31/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.6984 - acc: 0.7379 - recall: 0.6202 - precision: 0.8421 - f1: 0.7138 - val_loss: 0.8392 - val_acc: 0.6790 - val_recall: 0.5638 - val_precision: 0.8155 - val_f1: 0.6665\n",
            "Epoch 32/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 0.6843 - acc: 0.7420 - recall: 0.6295 - precision: 0.8413 - f1: 0.7200 - val_loss: 0.8347 - val_acc: 0.6790 - val_recall: 0.5761 - val_precision: 0.8194 - val_f1: 0.6761\n",
            "Epoch 33/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.6787 - acc: 0.7441 - recall: 0.6419 - precision: 0.8400 - f1: 0.7274 - val_loss: 0.8352 - val_acc: 0.6996 - val_recall: 0.6008 - val_precision: 0.8019 - val_f1: 0.6869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.6692 - acc: 0.7534 - recall: 0.6543 - precision: 0.8359 - f1: 0.7335 - val_loss: 0.8230 - val_acc: 0.6914 - val_recall: 0.6049 - val_precision: 0.8259 - val_f1: 0.6981\n",
            "Epoch 35/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.6585 - acc: 0.7554 - recall: 0.6584 - precision: 0.8383 - f1: 0.7375 - val_loss: 0.8160 - val_acc: 0.6872 - val_recall: 0.5967 - val_precision: 0.8197 - val_f1: 0.6902\n",
            "Epoch 36/100\n",
            "969/969 [==============================] - 0s 17us/step - loss: 0.6514 - acc: 0.7626 - recall: 0.6646 - precision: 0.8365 - f1: 0.7401 - val_loss: 0.8219 - val_acc: 0.6872 - val_recall: 0.6049 - val_precision: 0.8124 - val_f1: 0.6932\n",
            "Epoch 37/100\n",
            "969/969 [==============================] - 0s 21us/step - loss: 0.6399 - acc: 0.7606 - recall: 0.6811 - precision: 0.8397 - f1: 0.7515 - val_loss: 0.8222 - val_acc: 0.6914 - val_recall: 0.6008 - val_precision: 0.7978 - val_f1: 0.6852\n",
            "Epoch 38/100\n",
            "969/969 [==============================] - 0s 18us/step - loss: 0.6337 - acc: 0.7647 - recall: 0.6852 - precision: 0.8385 - f1: 0.7541 - val_loss: 0.8178 - val_acc: 0.6872 - val_recall: 0.6173 - val_precision: 0.7894 - val_f1: 0.6928\n",
            "Epoch 39/100\n",
            "969/969 [==============================] - 0s 19us/step - loss: 0.6266 - acc: 0.7688 - recall: 0.6914 - precision: 0.8408 - f1: 0.7588 - val_loss: 0.8125 - val_acc: 0.6872 - val_recall: 0.6049 - val_precision: 0.7989 - val_f1: 0.6883\n",
            "Epoch 40/100\n",
            "969/969 [==============================] - 0s 22us/step - loss: 0.6173 - acc: 0.7719 - recall: 0.6997 - precision: 0.8456 - f1: 0.7656 - val_loss: 0.8176 - val_acc: 0.6831 - val_recall: 0.6173 - val_precision: 0.7935 - val_f1: 0.6944\n",
            "Epoch 41/100\n",
            "969/969 [==============================] - 0s 20us/step - loss: 0.6111 - acc: 0.7761 - recall: 0.7007 - precision: 0.8476 - f1: 0.7672 - val_loss: 0.8113 - val_acc: 0.6872 - val_recall: 0.6132 - val_precision: 0.7968 - val_f1: 0.6929\n",
            "243/243 [==============================] - 0s 15us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRXHcqHHReZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.append([autoencoder_loss,m1_f_score,m2_f_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgb5PGl1ReZw",
        "colab_type": "code",
        "colab": {},
        "outputId": "ba0521d8-c24b-4190-ce8e-afe5137bf7cb"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.14784393274840163, 0.14135833722328453, 0.5432675618203089],\n",
              " [0.14784393274840163, 0.2186464696754644, 0.6593394166648142],\n",
              " [0.11737923866061596, 0.6959811667355981, 0.6981268029644656]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGyz80QzReZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}